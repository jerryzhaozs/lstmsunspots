{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "length of corpus is: 65000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many words: 380156\n",
      "tf-idf shape: (65000,380156)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlength of corpus is: 65000\\nhow many words: 379000\\ntf-idf shape: (65000,379000)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def constructDataset(path):\n",
    "    \"\"\"\n",
    "    path: file path\n",
    "    rtype: lable_list and corpus_list\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    corpus_list = []\n",
    "    with open(path, 'r', encoding='utf-8') as p:\n",
    "        for line in p.readlines():\n",
    "            label_list.append(line.split('\\t')[0])\n",
    "            corpus_list.append(line.split('\\t')[1])\n",
    "    return label_list, corpus_list\n",
    "    \n",
    "tmp_catalog = 'C:/Users/Administrator/Desktop/dogcat/cnews/'\n",
    "file_path = 'val_token.txt'\n",
    "val_label, val_set = constructDataset(tmp_catalog+file_path)\n",
    "print (len(val_set))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "tmp_catalog = 'C:/Users/Administrator/Desktop/dogcat/cnews/'\n",
    "write_list = [tmp_catalog+'train_token.txt', tmp_catalog+'test_token.txt']\n",
    "\n",
    "train_label, train_set = constructDataset(write_list[0]) # 50000\n",
    "test_label, test_set = constructDataset(write_list[1]) # 10000\n",
    "# 计算tf-idf\n",
    "corpus_set = train_set + val_set + test_set # 全量计算tf-idf\n",
    "print (\"length of corpus is: \" + str(len(corpus_set)))\n",
    "vectorizer = CountVectorizer(min_df=1e-5) # drop df < 1e-5,去低频词\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus_set))\n",
    "words = vectorizer.get_feature_names()\n",
    "print (\"how many words: {0}\".format(len(words)))\n",
    "print (\"tf-idf shape: ({0},{1})\".format(tfidf.shape[0], tfidf.shape[1]))\n",
    "\n",
    "\"\"\"\n",
    "length of corpus is: 65000\n",
    "how many words: 379000\n",
    "tf-idf shape: (65000,379000)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# encode label\n",
    "corpus_label = train_label + val_label + test_label\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "corpus_encode_label = encoder.fit_transform(corpus_label)\n",
    "# train_label = corpus_encode_label[:50000]\n",
    "# val_label = corpus_encode_label[50000:55000]\n",
    "# test_label = corpus_encode_label[55000:]\n",
    "# # get tf-idf dataset\n",
    "# train_set = tfidf[:50000]\n",
    "# val_set = tfidf[50000:55000]\n",
    "# test_set = tfidf[55000:]\n",
    "train_label_ = corpus_encode_label[:2000]\n",
    "val_label_ = corpus_encode_label[2000:3000]\n",
    "test_label_ = corpus_encode_label[3000:4000]\n",
    "# get tf-idf dataset\n",
    "train_set_ = tfidf[:2000]\n",
    "val_set_ = tfidf[2000:3000]\n",
    "test_set_ = tfidf[3000:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val mean accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier   \n",
    "rf_model = RandomForestClassifier(n_estimators=50, random_state=1080)\n",
    "rf_model.fit(train_set_, train_label_)\n",
    "print (\"val mean accuracy: {0}\".format(rf_model.score(val_set_, val_label_)))\n",
    "y_pred_ = rf_model.predict(test_set_)\n",
    "print (classification_report(test_label_, y_pred_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
